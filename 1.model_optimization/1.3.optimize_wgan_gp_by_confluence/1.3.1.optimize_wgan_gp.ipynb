{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path('.').absolute().parent.parent / \"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import virtual_stain_flow software "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/weishanli/Waylab/pediatric_cancer_atlas_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weishanli/anaconda3/envs/alsf_iqa/lib/python3.11/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.1'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(config['paths']['software_path'])\n",
    "print(str(pathlib.Path('.').absolute().parent.parent))\n",
    "\n",
    "## Dataset\n",
    "from virtual_stain_flow.datasets.PatchDataset import PatchDataset\n",
    "from virtual_stain_flow.datasets.CachedDataset import CachedDataset\n",
    "\n",
    "## wGaN training\n",
    "from virtual_stain_flow.models.unet import UNet\n",
    "from virtual_stain_flow.models.discriminator import GlobalDiscriminator\n",
    "from virtual_stain_flow.trainers.WGANTrainer import WGANTrainer\n",
    "\n",
    "## wGaN losses\n",
    "from virtual_stain_flow.losses.GradientPenaltyLoss import GradientPenaltyLoss\n",
    "from virtual_stain_flow.losses.DiscriminatorLoss import WassersteinLoss\n",
    "from virtual_stain_flow.losses.GeneratorLoss import GeneratorLoss\n",
    "\n",
    "from virtual_stain_flow.transforms.MinMaxNormalize import MinMaxNormalize\n",
    "from virtual_stain_flow.transforms.PixelDepthTransform import PixelDepthTransform\n",
    "\n",
    "## Metrics\n",
    "from virtual_stain_flow.metrics.MetricsWrapper import MetricsWrapper\n",
    "from virtual_stain_flow.metrics.PSNR import PSNR\n",
    "from virtual_stain_flow.metrics.SSIM import SSIM\n",
    "\n",
    "## callback\n",
    "from virtual_stain_flow.callbacks.MlflowLogger import MlflowLogger\n",
    "from virtual_stain_flow.callbacks.IntermediatePlot import IntermediatePlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths and other train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loaddata for train\n",
    "LOADDATA_FILE_PATH = pathlib.Path('.').absolute().parent.parent \\\n",
    "    / '0.data_preprocessing' / 'data_split_loaddata' / 'loaddata_train.csv'\n",
    "assert LOADDATA_FILE_PATH.exists()\n",
    "\n",
    "LOADDATA_HELDOUT_FILE_PATH = pathlib.Path('.').absolute().parent.parent \\\n",
    "    / '0.data_preprocessing' / 'data_split_loaddata' / 'loaddata_heldout.csv'\n",
    "assert LOADDATA_HELDOUT_FILE_PATH.exists(), f\"Directory not found: {LOADDATA_HELDOUT_FILE_PATH}\"\n",
    "\n",
    "SC_FEATURES_DIR = pathlib.Path(config['paths']['sc_features_path'])\n",
    "\n",
    "## Output directories\n",
    "MLFLOW_DIR = pathlib.Path('.').absolute() / 'optuna_mlflow'\n",
    "MLFLOW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OPTUNA_JOBLIB_DIR = pathlib.Path('.').absolute() / 'optuna_joblib'\n",
    "OPTUNA_JOBLIB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PLOT_DIR = pathlib.Path('.').absolute() / 'optuna_plots'\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## Basic data generation and max epoch definition\n",
    "PATCH_SIZE = 256\n",
    "EPOCHS = 1_000\n",
    "\n",
    "## Channels for input and target are read from config\n",
    "INPUT_CHANNEL_NAMES = config['data']['input_channel_keys']\n",
    "TARGET_CHANNEL_NAMES = config['data']['target_channel_keys']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines how the train data will be divided to train models on two levels of confluence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_GROUPING = {\n",
    "    'high_confluence': {\n",
    "        'seeding_density': [12_000, 8_000]\n",
    "    },\n",
    "    'low_confluence': {\n",
    "        'seeding_density': [4_000, 2_000, 1_000]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create patched dataset from heldout data for use with plotting predictions during optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaddata_heldout_df = pd.read_csv(LOADDATA_HELDOUT_FILE_PATH)\n",
    "## Retrieve relevant sc feature information\n",
    "sc_features = pd.DataFrame()\n",
    "for plate in loaddata_heldout_df['Metadata_Plate'].unique():\n",
    "    sc_features_parquet = SC_FEATURES_DIR / f'{plate}_sc_normalized.parquet'\n",
    "    if not sc_features_parquet.exists():\n",
    "        print(f'{sc_features_parquet} does not exist, skipping...')\n",
    "        continue \n",
    "    else:\n",
    "        sc_features = pd.concat([\n",
    "            sc_features, \n",
    "            pd.read_parquet(\n",
    "                sc_features_parquet,\n",
    "                columns=['Metadata_Plate', 'Metadata_Well', 'Metadata_Site', 'Metadata_Cells_Location_Center_X', 'Metadata_Cells_Location_Center_Y']\n",
    "            )\n",
    "        ])\n",
    "\n",
    "## Generate multi-channel patch dataset for plotting\n",
    "pds_heldout = PatchDataset(\n",
    "        _loaddata_csv=loaddata_heldout_df,\n",
    "        _sc_feature=sc_features,\n",
    "        _input_channel_keys=INPUT_CHANNEL_NAMES,\n",
    "        _target_channel_keys=TARGET_CHANNEL_NAMES,\n",
    "        _input_transform=PixelDepthTransform(src_bit_depth=16, target_bit_depth=8, _always_apply=True),\n",
    "        _target_transform=MinMaxNormalize(_normalization_factor=(2 ** 16) - 1, _always_apply=True),\n",
    "        patch_size=PATCH_SIZE,\n",
    "        verbose=False,\n",
    "        patch_generation_method=\"random_cell\",\n",
    "        n_expected_patches_per_img=5,\n",
    "        patch_generation_random_seed=42\n",
    "    )\n",
    "\n",
    "## Generate list of indice to plot\n",
    "n_patches = len(pds_heldout)\n",
    "random.seed(42)\n",
    "visualization_patch_indices = random.sample(range(n_patches), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimization objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def free_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def objective(trial, dataset, plot_dataset, channel_name, confluence_group_name, plot_dir):\n",
    "\n",
    "    trial_id = trial.number\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    gen_optim_lr = trial.suggest_float(\"gen_optim_lr\", 1e-5, 1e-2, log=True)\n",
    "    gen_optim_beta0 = trial.suggest_float(\"gen_optim_beta0\", 0.5, 0.9, log=False)\n",
    "    gen_optim_beta1 = trial.suggest_float('gen_optim_beta1', 0.9, 0.999, log=False)\n",
    "    gen_optim_weight_decay = 0 # no weight decay for generator optimizer\n",
    "    \n",
    "    disc_optim_lr = trial.suggest_float(\"disc_optim_lr\", 1e-5, 1e-2, log=True)\n",
    "    disc_optim_beta0 = trial.suggest_float(\"disc_optim_beta0\", 0.5, 0.9, log=False)\n",
    "    disc_optim_beta1 = trial.suggest_float('disc_optim_beta1', 0.9, 0.999, log=False)\n",
    "    disc_optim_weight_decay = trial.suggest_float('disc_optim_weight_decay', 1e-4, 1e-2, log=True)\n",
    "\n",
    "    # convolutional depth for unet generator model\n",
    "    gen_conv_depth = trial.suggest_int('gen_conv_depth', 3, 5) # convolutional depth for unet generator model\n",
    "    disc_conv_depth = trial.suggest_int('disc_conv_depth', 3, 5) # convolutional depth for discriminator network\n",
    "\n",
    "    # how often is the generator/discriminator weight updated (once every x epochs)\n",
    "    gen_update_freq = trial.suggest_int('gen_update_freq', 2, 8)\n",
    "    disc_update_freq = 1 # fixed for wGaN gp\n",
    "\n",
    "    # batch size and early stopping patience\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64, step=16)\n",
    "    patience = trial.suggest_int('patience', 5, 20) # early stop patience\n",
    "\n",
    "    ## Setup model, discriminator and optimizer\n",
    "    generator = UNet(\n",
    "        n_channels=1,\n",
    "        n_classes=1,\n",
    "        depth=gen_conv_depth,\n",
    "        bilinear=False\n",
    "    )\n",
    "    discriminator = GlobalDiscriminator(\n",
    "        n_in_channels = 2, # 1 input brightfield + 1 target fluo channel\n",
    "        n_in_filters = 64,\n",
    "        _conv_depth = disc_conv_depth,\n",
    "        _pool_before_fc = True\n",
    "    )\n",
    "\n",
    "    generator_optimizer = optim.Adam(generator.parameters(), \n",
    "                                 lr=gen_optim_lr, \n",
    "                                 betas=(gen_optim_beta0, gen_optim_beta1),\n",
    "                                 weight_decay=gen_optim_weight_decay)\n",
    "    \n",
    "    discriminator_optimizer = optim.Adam(discriminator.parameters(), \n",
    "                                        lr=disc_optim_lr, \n",
    "                                        betas=(disc_optim_beta0, disc_optim_beta1),\n",
    "                                        weight_decay=disc_optim_weight_decay)\n",
    "    \n",
    "    ## Metrics to be computed (and logged)\n",
    "    metric_fns = {\n",
    "        \"L1Loss\": MetricsWrapper(_metric_name='L1Loss', module=torch.nn.L1Loss()),\n",
    "        \"mse_loss\": MetricsWrapper(_metric_name='mse', module=torch.nn.MSELoss()),\n",
    "        \"ssim_loss\": SSIM(_metric_name=\"ssim\"),\n",
    "        \"psnr_loss\": PSNR(_metric_name=\"psnr\"),\n",
    "    }\n",
    "\n",
    "    ## Special losses\n",
    "\n",
    "    gp_loss = GradientPenaltyLoss(\n",
    "        _metric_name='gp_loss',\n",
    "        discriminator=discriminator,\n",
    "        weight=10.0,\n",
    "    )\n",
    "\n",
    "    gen_loss = GeneratorLoss(\n",
    "        _metric_name='gen_loss'\n",
    "    )\n",
    "\n",
    "    disc_loss = WassersteinLoss(\n",
    "        _metric_name='disc_loss'\n",
    "    )\n",
    "\n",
    "    ## Params to log with mlflow\n",
    "    params = {\n",
    "            # generation optimizer hyperparameters\n",
    "            \"gen_optim_lr\": gen_optim_lr,\n",
    "            \"gen_update_freq\": gen_update_freq,\n",
    "            \"gen_optim_beta0\": gen_optim_beta0,\n",
    "            \"gen_optim_beta1\": gen_optim_beta1,\n",
    "            \"gen_optim_weight_decay\": gen_optim_weight_decay,\n",
    "            # generator model hyperparameter(s)\n",
    "            \"gen_conv_depth\": gen_conv_depth,\n",
    "            # discriminator optimizer hyperparameters\n",
    "            \"disc_optim_lr\": disc_optim_lr,\n",
    "            \"disc_update_freq\": disc_update_freq,\n",
    "            \"disc_optim_beta0\": disc_optim_beta0,\n",
    "            \"disc_optim_beta1\": disc_optim_beta1,\n",
    "            \"disc_weight_decay\": disc_optim_weight_decay,\n",
    "            # discrminator model hyperparameter(s)\n",
    "            \"disc_conv_depth\": disc_conv_depth,\n",
    "            # dataset hyperparameters\n",
    "            \"patch_size\": PATCH_SIZE,\n",
    "            \"channel_name\": channel_name,\n",
    "            \"confluence\": confluence_group_name,\n",
    "            # data loader hyperparameters\n",
    "            \"batch_size\": batch_size,\n",
    "            # training hyperparameters\n",
    "            \"patience\": patience,\n",
    "            \"epochs\": EPOCHS,\n",
    "            # optuna trial id\n",
    "            \"trial_id\": trial_id\n",
    "        }\n",
    "\n",
    "    ## mlflow logger callback\n",
    "    mlflow_logger_callback = MlflowLogger(\n",
    "        name='mlflow_logger',\n",
    "        mlflow_uri=MLFLOW_DIR / 'mlruns',\n",
    "        mlflow_experiment_name=f'wGaN_gp_optimize_{confluence_group_name}',\n",
    "        mlflow_start_run_args={'run_name': f'wGaN_gp_optimize_{confluence_group_name}_{channel_name}', 'nested': True},\n",
    "        mlflow_log_params_args=params\n",
    "    )\n",
    "    \n",
    "    trial_plot_dir = pathlib.Path(plot_dir) / str(trial_id)\n",
    "    trial_plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    plot_callback = IntermediatePlot(\n",
    "            name='plotter',\n",
    "            path=trial_plot_dir,\n",
    "            dataset=plot_dataset,\n",
    "            indices=visualization_patch_indices, # every model being trained will have the same visualization patch indices\n",
    "            plot_metrics=[SSIM(_metric_name='ssim'), PSNR(_metric_name='psnr')],\n",
    "            figsize=(20, 25),\n",
    "            every_n_epochs=5,\n",
    "            show_plot=False,\n",
    "        )\n",
    "    \n",
    "    ## Trainer\n",
    "    wgan_trainer = WGANTrainer(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        gen_optimizer=generator_optimizer,\n",
    "        disc_optimizer=discriminator_optimizer,\n",
    "        generator_loss_fn=gen_loss,\n",
    "        discriminator_loss_fn=disc_loss,\n",
    "        gradient_penalty_fn=gp_loss,\n",
    "        discriminator_update_freq=disc_update_freq,\n",
    "        generator_update_freq=gen_update_freq,\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        epochs=EPOCHS,\n",
    "        patience=patience,\n",
    "        callbacks=[mlflow_logger_callback, plot_callback],\n",
    "        metrics=metric_fns,\n",
    "        device='cuda',\n",
    "        early_termination_metric='L1Loss' # Early termination and optimization will be based on L1 loss\n",
    "    )\n",
    "\n",
    "    # Train the model and log validation loss\n",
    "    wgan_trainer.train()\n",
    "    val_loss = wgan_trainer.best_loss\n",
    "\n",
    "    del generator\n",
    "    del discriminator\n",
    "\n",
    "    del generator_optimizer\n",
    "    del discriminator_optimizer\n",
    "\n",
    "    del gp_loss\n",
    "    del gen_loss\n",
    "    del disc_loss\n",
    "\n",
    "    del wgan_trainer\n",
    "    \n",
    "    free_gpu_memory()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize for wGAN GP hyperparameters per Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning optimization for channel: OrigDNA for high_confluence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-10 10:04:46,481] Trial 5 failed with parameters: {'gen_optim_lr': 0.00014656553886225324, 'gen_optim_beta0': 0.6085396127095584, 'gen_optim_beta1': 0.982045013406041, 'disc_optim_lr': 0.00011756010900231849, 'disc_optim_beta0': 0.6123738038749523, 'disc_optim_beta1': 0.9537269122326666, 'disc_optim_weight_decay': 0.00019135880487692312, 'gen_conv_depth': 5, 'disc_conv_depth': 3, 'gen_update_freq': 8, 'batch_size': 64, 'patience': 8} because of the following error: ValueError('Shape mismatch: predictions torch.Size([5, 1, 256, 256]) vs targets torch.Size([5, 5, 256, 256])').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/weishanli/anaconda3/envs/alsf_iqa/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1295833/3269007778.py\", line 75, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, cds, channel_name, confluence_group_name, plot_dir), n_trials=1)\n",
      "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_1295833/2000130912.py\", line 157, in objective\n",
      "    wgan_trainer.train()\n",
      "  File \"/home/weishanli/Waylab/virtual_stain_flow/trainers/WGANTrainer.py\", line 254, in train\n",
      "    super().train()\n",
      "  File \"/home/weishanli/Waylab/virtual_stain_flow/trainers/AbstractTrainer.py\", line 210, in train\n",
      "    callback.on_epoch_end()\n",
      "  File \"/home/weishanli/Waylab/virtual_stain_flow/callbacks/IntermediatePlot.py\", line 90, in on_epoch_end\n",
      "    self._plot()\n",
      "  File \"/home/weishanli/Waylab/virtual_stain_flow/callbacks/IntermediatePlot.py\", line 108, in _plot\n",
      "    plot_predictions_grid_from_model(\n",
      "  File \"/home/weishanli/Waylab/virtual_stain_flow/evaluation/visualization_utils.py\", line 154, in plot_predictions_grid_from_model\n",
      "    metrics_df = evaluate_per_image_metric(predictions, targets, metrics)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/weishanli/Waylab/virtual_stain_flow/evaluation/evaluation_utils.py\", line 30, in evaluate_per_image_metric\n",
      "    raise ValueError(f\"Shape mismatch: predictions {predictions.shape} vs targets {targets.shape}\")\n",
      "ValueError: Shape mismatch: predictions torch.Size([5, 1, 256, 256]) vs targets torch.Size([5, 5, 256, 256])\n",
      "[W 2025-03-10 10:04:46,482] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: predictions torch.Size([5, 1, 256, 256]) vs targets torch.Size([5, 5, 256, 256])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Resume optimization and run up until N_TRIALS\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m<\u001b[39m N_TRIALS:\n\u001b[0;32m---> 75\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, cds, channel_name, confluence_group_name, plot_dir), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(study, study_path)  \u001b[38;5;66;03m# Save study after every trial\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved study after trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_TRIALS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/alsf_iqa/lib/python3.11/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     _optimize(\n\u001b[1;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    485\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/alsf_iqa/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[1;32m     64\u001b[0m             study,\n\u001b[1;32m     65\u001b[0m             func,\n\u001b[1;32m     66\u001b[0m             n_trials,\n\u001b[1;32m     67\u001b[0m             timeout,\n\u001b[1;32m     68\u001b[0m             catch,\n\u001b[1;32m     69\u001b[0m             callbacks,\n\u001b[1;32m     70\u001b[0m             gc_after_trial,\n\u001b[1;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/alsf_iqa/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/alsf_iqa/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/alsf_iqa/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[8], line 75\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Resume optimization and run up until N_TRIALS\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m<\u001b[39m N_TRIALS:\n\u001b[0;32m---> 75\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(\u001b[38;5;28;01mlambda\u001b[39;00m trial: objective(trial, cds, channel_name, confluence_group_name, plot_dir), n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(study, study_path)  \u001b[38;5;66;03m# Save study after every trial\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved study after trial \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(study\u001b[38;5;241m.\u001b[39mtrials)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_TRIALS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 157\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, dataset, channel_name, confluence_group_name, plot_dir)\u001b[0m\n\u001b[1;32m    136\u001b[0m wgan_trainer \u001b[38;5;241m=\u001b[39m WGANTrainer(\n\u001b[1;32m    137\u001b[0m     generator\u001b[38;5;241m=\u001b[39mgenerator,\n\u001b[1;32m    138\u001b[0m     discriminator\u001b[38;5;241m=\u001b[39mdiscriminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     early_termination_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL1Loss\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Early termination and optimization will be based on L1 loss\u001b[39;00m\n\u001b[1;32m    154\u001b[0m )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Train the model and log validation loss\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m wgan_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    158\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m wgan_trainer\u001b[38;5;241m.\u001b[39mbest_loss\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m generator\n",
      "File \u001b[0;32m~/Waylab/virtual_stain_flow/trainers/WGANTrainer.py:254\u001b[0m, in \u001b[0;36mWGANTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discriminator\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Waylab/virtual_stain_flow/trainers/AbstractTrainer.py:210\u001b[0m, in \u001b[0;36mAbstractTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Invoke callback on epoch_end\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 210\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_end()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Update early stopping\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_early_termination_metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# Do not perform early stopping when no termination metric is specified\u001b[39;00m\n",
      "File \u001b[0;32m~/Waylab/virtual_stain_flow/callbacks/IntermediatePlot.py:90\u001b[0m, in \u001b[0;36mIntermediatePlot.on_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03mCalled at the end of each epoch to plot predictions if the epoch is a multiple of `every_n_epochs`.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevery_n_epochs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mepoch:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plot()\n",
      "File \u001b[0;32m~/Waylab/virtual_stain_flow/callbacks/IntermediatePlot.py:108\u001b[0m, in \u001b[0;36mIntermediatePlot._plot\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03mHelper method to generate and save plots.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mPlot dataset with model predictions on n random images from dataset at the end of each epoch.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03mCalled by the on_epoch_end and on_train_end methods\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    106\u001b[0m original_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 108\u001b[0m plot_predictions_grid_from_model(\n\u001b[1;32m    109\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    110\u001b[0m     dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset,\n\u001b[1;32m    111\u001b[0m     indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_subset_indices,\n\u001b[1;32m    112\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_metrics,\n\u001b[1;32m    113\u001b[0m     save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    114\u001b[0m     device\u001b[38;5;241m=\u001b[39moriginal_device,\n\u001b[1;32m    115\u001b[0m     show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplot_kwargs\n\u001b[1;32m    117\u001b[0m )\n",
      "File \u001b[0;32m~/Waylab/virtual_stain_flow/evaluation/visualization_utils.py:154\u001b[0m, in \u001b[0;36mplot_predictions_grid_from_model\u001b[0;34m(model, dataset, indices, metrics, device, save_path, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m targets, predictions \u001b[38;5;241m=\u001b[39m predict_image(dataset, model, indices\u001b[38;5;241m=\u001b[39mindices, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Step 2: Compute per-image metrics for the subset\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m metrics_df \u001b[38;5;241m=\u001b[39m evaluate_per_image_metric(predictions, targets, metrics)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# Step 3: Extract subset of inputs & targets and plot\u001b[39;00m\n\u001b[1;32m    157\u001b[0m is_patch_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(dataset, PatchDataset)\n",
      "File \u001b[0;32m~/Waylab/virtual_stain_flow/evaluation/evaluation_utils.py:30\u001b[0m, in \u001b[0;36mevaluate_per_image_metric\u001b[0;34m(predictions, targets, metrics, indices)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mComputes a set of metrics on a per-image basis and returns the results as a pandas DataFrame.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m:rtype: pd.DataFrame\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m targets\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: predictions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictions\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs targets \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtargets\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Shape mismatch: predictions torch.Size([5, 1, 256, 256]) vs targets torch.Size([5, 5, 256, 256])"
     ]
    }
   ],
   "source": [
    "N_TRIALS = 50  # Total trials\n",
    "\n",
    "## Loaddata for optimization\n",
    "loaddata_df = pd.read_csv(LOADDATA_FILE_PATH)\n",
    "for confluence_group_name, conditions in DATA_GROUPING.items():\n",
    "\n",
    "    ## Loaddata for the confluence group\n",
    "    loaddata_condition_df = loaddata_df.copy()\n",
    "    for condition, values in conditions.items():\n",
    "        loaddata_condition_df = loaddata_condition_df[\n",
    "            loaddata_condition_df[condition].isin(values)\n",
    "        ]\n",
    "\n",
    "    ## Retrieve relevant sc features by assemblying them from parquet files\n",
    "    sc_features = pd.DataFrame()\n",
    "    for plate in loaddata_condition_df['Metadata_Plate'].unique():\n",
    "        sc_features_parquet = SC_FEATURES_DIR / f'{plate}_sc_normalized.parquet'\n",
    "        if not sc_features_parquet.exists():\n",
    "            print(f'{sc_features_parquet} does not exist, skipping...')\n",
    "            continue \n",
    "        else:\n",
    "            sc_features = pd.concat([\n",
    "                sc_features, \n",
    "                pd.read_parquet(\n",
    "                    sc_features_parquet,\n",
    "                    columns=['Metadata_Plate', 'Metadata_Well', 'Metadata_Site', 'Metadata_Cells_Location_Center_X', 'Metadata_Cells_Location_Center_Y']\n",
    "                )\n",
    "            ])\n",
    "\n",
    "    ## Create patch dataset\n",
    "    pds = PatchDataset(\n",
    "        _loaddata_csv=loaddata_condition_df,\n",
    "        _sc_feature=sc_features,\n",
    "        _input_channel_keys=INPUT_CHANNEL_NAMES,\n",
    "        _target_channel_keys=TARGET_CHANNEL_NAMES,\n",
    "        _input_transform=PixelDepthTransform(src_bit_depth=16, target_bit_depth=8, _always_apply=True),\n",
    "        _target_transform=MinMaxNormalize(_normalization_factor=(2 ** 16) - 1, _always_apply=True),\n",
    "        patch_size=PATCH_SIZE,\n",
    "        verbose=False,\n",
    "        patch_generation_method=\"random_cell\",\n",
    "        patch_generation_random_seed=42\n",
    "    )\n",
    "\n",
    "    for channel_name in TARGET_CHANNEL_NAMES:\n",
    "\n",
    "        # Create directory for plots\n",
    "        plot_dir = PLOT_DIR / f\"{confluence_group_name}_{channel_name}\"\n",
    "        plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        ## Configure dataset channel\n",
    "        pds.set_input_channel_keys(INPUT_CHANNEL_NAMES)\n",
    "        pds.set_target_channel_keys(channel_name)\n",
    "\n",
    "        # Configure heldout dataset channel\n",
    "        pds_heldout.set_input_channel_keys(INPUT_CHANNEL_NAMES)\n",
    "        pds_heldout.set_target_channel_keys(channel_name)\n",
    "\n",
    "        ## Cache dataset of channel\n",
    "        cds = CachedDataset(\n",
    "            dataset=pds,\n",
    "            prefill_cache=True\n",
    "        )\n",
    "\n",
    "        print(f\"Beginning optimization for channel: {channel_name} for {confluence_group_name}\")\n",
    "\n",
    "        # Load the existing study for the current channel\n",
    "        study_path = OPTUNA_JOBLIB_DIR / f\"wGaN_gp_optimize_{channel_name}_{confluence_group_name}.joblib\"\n",
    "        if study_path.exists():\n",
    "            study = joblib.load(study_path)\n",
    "        else:\n",
    "            study = optuna.create_study(\n",
    "                direction=\"minimize\",\n",
    "                study_name=f\"wGaN_gp_optimize_{channel_name}_{confluence_group_name}\",\n",
    "                sampler=optuna.samplers.TPESampler(seed=42)\n",
    "            )\n",
    "\n",
    "        # Resume optimization and run up until N_TRIALS\n",
    "        while len(study.trials) < N_TRIALS:\n",
    "            study.optimize(lambda trial: objective(trial, cds, pds_heldout, channel_name, confluence_group_name, plot_dir), n_trials=1)\n",
    "            joblib.dump(study, study_path)  # Save study after every trial\n",
    "            print(f\"Saved study after trial {len(study.trials)}/{N_TRIALS}\")\n",
    "        \n",
    "        print(f\"{N_TRIALS} of Hyperparameter Optimization for {channel_name}_{confluence_group_name} completed.\")\n",
    "\n",
    "        # Print best trial results\n",
    "        print(f\"Best trial for channel {channel_name}:\")\n",
    "        print(f\"  Validation Loss: {study.best_trial.value}\")\n",
    "        print(f\"  Hyperparameters: {study.best_trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alsf_iqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
