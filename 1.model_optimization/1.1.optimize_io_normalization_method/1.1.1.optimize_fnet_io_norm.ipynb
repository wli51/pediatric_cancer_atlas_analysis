{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook runs optimization experiments on different combination of input/target normalization image transforms using the FNet model architecture by fixing hyper parameters during training to understand the effect of normalization methods on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import yaml\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config\n",
    "Paths to image data/metadata and dependent software location, as well as channel information are obtained from the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path('.').absolute().parent.parent / \"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import virtual_stain_flow software "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/weishanli/Waylab/pediatric_cancer_atlas_analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weishanli/anaconda3/envs/speckle_analysis/lib/python3.11/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.5' (you have '2.0.4'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(config['paths']['software_path'])\n",
    "print(str(pathlib.Path('.').absolute().parent.parent))\n",
    "\n",
    "## Dataset\n",
    "from virtual_stain_flow.datasets.PatchDataset import PatchDataset\n",
    "from virtual_stain_flow.datasets.CachedDataset import CachedDataset\n",
    "\n",
    "## FNet training\n",
    "from virtual_stain_flow.models.fnet import FNet\n",
    "from virtual_stain_flow.trainers.Trainer import Trainer\n",
    "\n",
    "from virtual_stain_flow.transforms.MinMaxNormalize import MinMaxNormalize\n",
    "from virtual_stain_flow.transforms.PixelDepthTransform import PixelDepthTransform\n",
    "from virtual_stain_flow.transforms.ZScoreNormalize import ZScoreNormalize\n",
    "\n",
    "## Metrics\n",
    "from virtual_stain_flow.metrics.MetricsWrapper import MetricsWrapper\n",
    "from virtual_stain_flow.metrics.PSNR import PSNR\n",
    "from virtual_stain_flow.metrics.SSIM import SSIM\n",
    "\n",
    "## callback\n",
    "from virtual_stain_flow.callbacks.MlflowLogger import MlflowLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths and other train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loaddata split for train is also used for optimization\n",
    "LOADDATA_FILE_PATH = pathlib.Path('.').absolute().parent.parent \\\n",
    "    / '0.data_preprocessing' / 'data_split_loaddata' / 'loaddata_train.csv'\n",
    "assert LOADDATA_FILE_PATH.exists(), f\"File not found: {LOADDATA_FILE_PATH}\"\n",
    "\n",
    "SC_FEATURES_DIR = pathlib.Path(config['paths']['sc_features_path'])\n",
    "assert SC_FEATURES_DIR.exists(), f\"Directory not found: {SC_FEATURES_DIR}\"\n",
    "\n",
    "## Output directories\n",
    "MLFLOW_DIR = pathlib.Path('.').absolute() / 'optuna_mlflow'\n",
    "MLFLOW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# dump directory for optuna studies\n",
    "OPTUNA_JOBLIB_DIR = pathlib.Path('.').absolute() / 'optuna_joblib'\n",
    "OPTUNA_JOBLIB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## Basic data generation, model convolutional depth, optimizer param and max epoch definition\n",
    "# Will be using these fixed values for the normalization method optimization\n",
    "PATCH_SIZE = 256\n",
    "CONV_DEPTH = 5\n",
    "LR = 1e-4\n",
    "BETAS = (0.5, 0.9)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1_000\n",
    "PATIENCE = 20\n",
    "\n",
    "## Channels for input and target are read from config\n",
    "INPUT_CHANNEL_NAMES = config['data']['input_channel_keys']\n",
    "TARGET_CHANNEL_NAMES = config['data']['target_channel_keys']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Normalization Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define transforms and parameters\n",
    "NORM_METHODS = {\n",
    "    \"z_score\": {\n",
    "        \"class\": ZScoreNormalize,\n",
    "        \"args\": {\"_mean\": None, \"_std\": None, \"_always_apply\": True, \"_p\": 1.0}\n",
    "    },\n",
    "    \"8bit\": {\n",
    "        \"class\": PixelDepthTransform,\n",
    "        \"args\": {\"src_bit_depth\": 16, \"target_bit_depth\": 8, \"_always_apply\": True, \"_p\": 1.0}\n",
    "    },\n",
    "    \"min_max\": {\n",
    "        \"class\": MinMaxNormalize,\n",
    "        \"args\": {\"_normalization_factor\": (2 ** 16) - 1, \"_always_apply\": True, \"_p\": 1.0}\n",
    "    }\n",
    "}\n",
    "\n",
    "## Define the model output activation to be used with each output normalization\n",
    "NORM_METHOD_ACTIVATION = {\n",
    "    \"z_score\": \"linear\",\n",
    "    \"8bit\": \"linear\",\n",
    "    \"min_max\": \"sigmoid\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimization objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DATA = True\n",
    "\n",
    "def free_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def objective(trial, dataset, channel_name):\n",
    "\n",
    "    # Suggest an input and targettransform\n",
    "    input_transform = trial.suggest_categorical(\"input_transform\", list(NORM_METHODS.keys()))\n",
    "    target_transform = trial.suggest_categorical(\"target_transform\", list(NORM_METHODS.keys()))\n",
    "\n",
    "    ## Configure the dataset with normalization methods\n",
    "    dataset.set_input_transform(NORM_METHODS[input_transform][\"class\"](**NORM_METHODS[input_transform][\"args\"]))\n",
    "    dataset.set_target_transform(NORM_METHODS[target_transform][\"class\"](**NORM_METHODS[target_transform][\"args\"]))\n",
    "\n",
    "    ## Cache dataset\n",
    "    # Caching PatchDatasets (into RAM) can substantially improve training speed, mostly\n",
    "    # due to speeding up the data shuffling process that can be slow with dynamically\n",
    "    # cropping patches from large images. However, to really benefit from caching it is\n",
    "    # necessary to use a cache size that fits the entire dataset (or close to doing so).\n",
    "    # Consider not using the Cached Dataset if memory is limited. Training/optimization is \n",
    "    # completely functional with the dynamic PatchDataset.\n",
    "    if CACHE_DATA:\n",
    "        dataset = CachedDataset(\n",
    "                dataset=dataset,\n",
    "                prefill_cache=True\n",
    "            )\n",
    "    else:\n",
    "        # uses the dynamic PatchDataset\n",
    "        pass\n",
    "\n",
    "    ## Setup model and optimizer\n",
    "    model = FNet(depth=CONV_DEPTH, \n",
    "                 # output activation paired with target/output normalization\n",
    "                 output_activation=NORM_METHOD_ACTIVATION[target_transform])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, betas=BETAS)\n",
    "    \n",
    "    ## Metrics to be computed (and logged)\n",
    "    metric_fns = {\n",
    "        \"mse_loss\": MetricsWrapper(_metric_name='mse', module=torch.nn.MSELoss()),\n",
    "        \"ssim_loss\": SSIM(_metric_name=\"ssim\"),\n",
    "        \"psnr_loss\": PSNR(_metric_name=\"psnr\"),\n",
    "    }\n",
    "\n",
    "    ## Params to log with mlflow\n",
    "    params = {\n",
    "            \"lr\": LR,\n",
    "            \"beta0\": BETAS[0],\n",
    "            \"beta1\": BETAS[1],\n",
    "            \"depth\": CONV_DEPTH,\n",
    "            \"patch_size\": PATCH_SIZE,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"patience\": PATIENCE,\n",
    "            \"input_norm\": input_transform,\n",
    "            \"target_norm\": target_transform,\n",
    "            \"channel_name\": channel_name,\n",
    "        }\n",
    "\n",
    "    ## mlflow logger callback\n",
    "    mlflow_logger_callback = MlflowLogger(\n",
    "        name='mlflow_logger',\n",
    "        mlflow_uri=MLFLOW_DIR / 'mlruns',\n",
    "        mlflow_experiment_name='FNet_optimize_io_norm',\n",
    "        mlflow_start_run_args={'run_name': f'FNet_optimize_io_norm_{channel_name}', 'nested': True},\n",
    "        mlflow_log_params_args=params\n",
    "    )\n",
    "    \n",
    "    ## Trainer\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        optimizer = optimizer,\n",
    "        backprop_loss = torch.nn.L1Loss(), # MAE loss for backpropagation\n",
    "        dataset = dataset,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        epochs = EPOCHS,\n",
    "        patience = PATIENCE,\n",
    "        callbacks=[mlflow_logger_callback],\n",
    "        metrics=metric_fns,\n",
    "        device = 'cuda',\n",
    "        early_termination_metric='L1Loss'\n",
    "    )\n",
    "\n",
    "    # Train the model and log validation loss\n",
    "    trainer.train()\n",
    "    val_loss = trainer.best_loss\n",
    "\n",
    "    del model\n",
    "    del optimizer\n",
    "    del metric_fns\n",
    "    del mlflow_logger_callback\n",
    "    del trainer\n",
    "    \n",
    "    free_gpu_memory()\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize for I/O normalizationm method per Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning optimization for channel: OrigDNA for io normalization methods\n",
      "50 of Normalization Method Optimization for OrigDNA completed.\n",
      "Best trial for channel OrigDNA:\n",
      "  Validation Loss: 0.0048707767855376005\n",
      "  Hyperparameters: {'input_transform': 'min_max', 'target_transform': 'min_max'}\n",
      "Beginning optimization for channel: OrigER for io normalization methods\n",
      "50 of Normalization Method Optimization for OrigER completed.\n",
      "Best trial for channel OrigER:\n",
      "  Validation Loss: 0.004642733547370881\n",
      "  Hyperparameters: {'input_transform': 'z_score', 'target_transform': 'min_max'}\n",
      "Beginning optimization for channel: OrigAGP for io normalization methods\n",
      "50 of Normalization Method Optimization for OrigAGP completed.\n",
      "Best trial for channel OrigAGP:\n",
      "  Validation Loss: 0.0012983351480215788\n",
      "  Hyperparameters: {'input_transform': 'z_score', 'target_transform': 'min_max'}\n",
      "Beginning optimization for channel: OrigMito for io normalization methods\n",
      "50 of Normalization Method Optimization for OrigMito completed.\n",
      "Best trial for channel OrigMito:\n",
      "  Validation Loss: 0.026521524880081415\n",
      "  Hyperparameters: {'input_transform': 'min_max', 'target_transform': 'min_max'}\n",
      "Beginning optimization for channel: OrigRNA for io normalization methods\n",
      "50 of Normalization Method Optimization for OrigRNA completed.\n",
      "Best trial for channel OrigRNA:\n",
      "  Validation Loss: 0.0049406999023631215\n",
      "  Hyperparameters: {'input_transform': '8bit', 'target_transform': 'min_max'}\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS = 50\n",
    "\n",
    "## Loaddata for optimization\n",
    "loaddata_df = pd.read_csv(LOADDATA_FILE_PATH)\n",
    "sc_features = pd.DataFrame()\n",
    "\n",
    "## Retrieve relevant sc features by assemblying them from parquet files\n",
    "for plate in loaddata_df['Metadata_Plate'].unique():\n",
    "    sc_features_parquet = SC_FEATURES_DIR / f'{plate}_sc_normalized.parquet'\n",
    "    if not sc_features_parquet.exists():\n",
    "        print(f'{sc_features_parquet} does not exist, skipping...')\n",
    "        continue \n",
    "    else:\n",
    "        sc_features = pd.concat([\n",
    "            sc_features, \n",
    "            pd.read_parquet(\n",
    "                sc_features_parquet,\n",
    "                columns=['Metadata_Plate', 'Metadata_Well', 'Metadata_Site', 'Metadata_Cells_Location_Center_X', 'Metadata_Cells_Location_Center_Y']\n",
    "            )\n",
    "        ])\n",
    "\n",
    "## Create patch dataset\n",
    "pds = PatchDataset(\n",
    "        _loaddata_csv=loaddata_df,\n",
    "        _sc_feature=sc_features,\n",
    "        _input_channel_keys=INPUT_CHANNEL_NAMES,\n",
    "        _target_channel_keys=TARGET_CHANNEL_NAMES,\n",
    "        _input_transform=None,\n",
    "        _target_transform=None,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        verbose=False,\n",
    "        patch_generation_method=\"random_cell\",\n",
    "        n_expected_patches_per_img=50,\n",
    "        patch_generation_random_seed=42\n",
    "    )\n",
    "\n",
    "for channel_name in TARGET_CHANNEL_NAMES:\n",
    "\n",
    "    ## Configure dataset channel\n",
    "    pds.set_input_channel_keys(INPUT_CHANNEL_NAMES)\n",
    "    pds.set_target_channel_keys(channel_name)\n",
    "    ## Caching of dataset is handled within the objective function due \n",
    "    ## to the need to change normalization methods for each trial\n",
    "\n",
    "    print(f\"Beginning optimization for channel: {channel_name} for io normalization methods\")\n",
    "\n",
    "    # Load the existing study\n",
    "    study_path = OPTUNA_JOBLIB_DIR / f\"FNet_optimize_{channel_name}_io_norm.joblib\"\n",
    "    if study_path.exists():\n",
    "        study = joblib.load(study_path)\n",
    "    else:\n",
    "        # Or create if not already existing\n",
    "        study = optuna.create_study(\n",
    "            direction=\"minimize\",\n",
    "            study_name=f\"FNet_optimize_{channel_name}_io_norm\",\n",
    "            sampler=optuna.samplers.TPESampler(seed=42)\n",
    "        )\n",
    "\n",
    "    # Resume optimization and run up until N_TRIALS\n",
    "    while len(study.trials) < N_TRIALS:\n",
    "        study.optimize(lambda trial: objective(trial, pds, channel_name), n_trials=1)\n",
    "        joblib.dump(study, study_path)\n",
    "        print(f\"Saved study after trial {len(study.trials)}/{N_TRIALS}\")\n",
    "    \n",
    "    print(f\"{N_TRIALS} of Normalization Method Optimization for {channel_name} completed.\")\n",
    "\n",
    "    # Print best trial results\n",
    "    print(f\"Best trial for channel {channel_name}:\")\n",
    "    print(f\"  Validation Loss: {study.best_trial.value}\")\n",
    "    print(f\"  Hyperparameters: {study.best_trial.params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speckle_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
