{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook trains FNet model on two datasets (high and low confluence level) of U2-OS cell painting image data with optimal hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import optuna\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path('.').absolute().parent.parent / \"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import virtual_stain_flow software "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/weishanli/Waylab/pediatric_cancer_atlas_analysis\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(config['paths']['software_path'])\n",
    "print(str(pathlib.Path('.').absolute().parent.parent))\n",
    "\n",
    "## Dataset\n",
    "from virtual_stain_flow.datasets.PatchDataset import PatchDataset\n",
    "from virtual_stain_flow.datasets.CachedDataset import CachedDataset\n",
    "\n",
    "## FNet training\n",
    "from virtual_stain_flow.models.fnet import FNet\n",
    "from virtual_stain_flow.trainers.Trainer import Trainer\n",
    "\n",
    "## wGaN training\n",
    "from virtual_stain_flow.models.unet import UNet\n",
    "from virtual_stain_flow.models.discriminator import GlobalDiscriminator\n",
    "from virtual_stain_flow.trainers.WGaNTrainer import WGaNTrainer\n",
    "\n",
    "## wGaN losses\n",
    "from virtual_stain_flow.losses.GradientPenaltyLoss import GradientPenaltyLoss\n",
    "from virtual_stain_flow.losses.DiscriminatorLoss import DiscriminatorLoss\n",
    "from virtual_stain_flow.losses.GeneratorLoss import GeneratorLoss\n",
    "\n",
    "from virtual_stain_flow.transforms.MinMaxNormalize import MinMaxNormalize\n",
    "from virtual_stain_flow.transforms.PixelDepthTransform import PixelDepthTransform\n",
    "\n",
    "## Metrics\n",
    "from virtual_stain_flow.metrics.MetricsWrapper import MetricsWrapper\n",
    "from virtual_stain_flow.metrics.PSNR import PSNR\n",
    "from virtual_stain_flow.metrics.SSIM import SSIM\n",
    "\n",
    "## callback\n",
    "from virtual_stain_flow.callbacks.MlflowLogger import MlflowLogger\n",
    "from virtual_stain_flow.callbacks.IntermediatePlot import IntermediatePatchPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths and other train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimization results \n",
    "OPTUNA_RESULTS_DIR = pathlib.Path('.').absolute().parent.parent / \\\n",
    "    '1.model_optimization' / \\\n",
    "    '1.2.optimize_fnet_by_confluence' / \\\n",
    "    'optuna_joblib'\n",
    "assert OPTUNA_RESULTS_DIR.exists()\n",
    "\n",
    "## Loaddata for train\n",
    "LOADDATA_FILE_PATH = pathlib.Path('.').absolute().parent.parent \\\n",
    "    / '0.data_preprocessing' / 'data_split_loaddata' / 'loaddata_train.csv'\n",
    "assert LOADDATA_FILE_PATH.exists()\n",
    "SC_FEATURES_DIR = pathlib.Path(config['paths']['sc_features_path'])\n",
    "\n",
    "## Train Logging/Weight output directory\n",
    "MLFLOW_DIR = pathlib.Path('.').absolute() / 'mlflow'\n",
    "MLFLOW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## Train intermediate plot otuput directory\n",
    "PLOT_DIR = pathlib.Path('.').absolute() / 'plots'\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "## Basic data generation, model convolutional depth and max epoch definition\n",
    "PATCH_SIZE = 256\n",
    "EPOCHS = 1_000\n",
    "\n",
    "## Channels for input and target are read from config\n",
    "INPUT_CHANNEL_NAMES = config['data']['input_channel_keys']\n",
    "TARGET_CHANNEL_NAMES = config['data']['target_channel_keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_name = \"OrigAGP\"\n",
    "confluence_group_name = \"high_confluence\"\n",
    "study_path = OPTUNA_RESULTS_DIR / \\\n",
    "            f\"FNet_optimize_{channel_name}_{confluence_group_name}.joblib\"\n",
    "if study_path.exists():\n",
    "    study = joblib.load(study_path)\n",
    "else:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0005690482786963295,\n",
       " 'beta0': 0.8951555358866697,\n",
       " 'beta1': 0.9770467004930141,\n",
       " 'batch_size': 32,\n",
       " 'patience': 15,\n",
       " 'conv_depth': 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = study.best_params['lr']\n",
    "beta0 = study.best_params['beta0']\n",
    "beta1 = study.best_params['beta1']\n",
    "patience = study.best_params['patience']\n",
    "batch_size = study.best_params['batch_size']\n",
    "conv_depth = study.best_params['conv_depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines how the train data will be divided to train models on two levels of confluence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_GROUPING = {\n",
    "    'high_confluence': {\n",
    "        'seeding_density': [12_000, 8_000]\n",
    "    },\n",
    "    'low_confluence': {\n",
    "        'seeding_density': [4_000, 2_000, 1_000]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_fns = {\n",
    "        \"mse_loss\": MetricsWrapper(_metric_name='mse', module=torch.nn.MSELoss()),\n",
    "        \"ssim_loss\": SSIM(_metric_name=\"ssim\"),\n",
    "        \"psnr_loss\": PSNR(_metric_name=\"psnr\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train seed and device settings for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seed = 42\n",
    "TRAIN_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "random.seed(train_seed)\n",
    "np.random.seed(train_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with best best hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning FNet model training for channel: OrigDNA and group high_confluence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/26 23:18:41 INFO mlflow.tracking.fluent: Experiment with name 'FNet_train_high_confluence' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early termination at epoch 41 with best validation metric 0.019157148897647858\n",
      "Beginning FNet model training for channel: OrigER and group high_confluence\n",
      "Early termination at epoch 48 with best validation metric 0.022135975903698375\n",
      "Beginning FNet model training for channel: OrigAGP and group high_confluence\n",
      "Early termination at epoch 60 with best validation metric 0.0030946837339018074\n",
      "Beginning FNet model training for channel: OrigMito and group high_confluence\n",
      "Early termination at epoch 23 with best validation metric 0.06919396238831374\n",
      "Beginning FNet model training for channel: OrigRNA and group high_confluence\n",
      "Early termination at epoch 29 with best validation metric 0.019445359563598268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/27 00:07:37 INFO mlflow.tracking.fluent: Experiment with name 'FNet_train_low_confluence' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning FNet model training for channel: OrigDNA and group low_confluence\n",
      "Early termination at epoch 34 with best validation metric 0.0074959140712101205\n",
      "Beginning FNet model training for channel: OrigER and group low_confluence\n",
      "Early termination at epoch 31 with best validation metric 0.015339045001095846\n",
      "Beginning FNet model training for channel: OrigAGP and group low_confluence\n",
      "Early termination at epoch 76 with best validation metric 0.0017006185003801395\n",
      "Beginning FNet model training for channel: OrigMito and group low_confluence\n",
      "Early termination at epoch 38 with best validation metric 0.0391944024179663\n",
      "Beginning FNet model training for channel: OrigRNA and group low_confluence\n",
      "Early termination at epoch 53 with best validation metric 0.010584966818753042\n"
     ]
    }
   ],
   "source": [
    "loaddata_df = pd.read_csv(LOADDATA_FILE_PATH)\n",
    "\n",
    "## Iterate over confluence (as separate optimizations were performed for each confluence level)\n",
    "for confluence_group_name, conditions in DATA_GROUPING.items():\n",
    "\n",
    "    ## Load dataset\n",
    "    loaddata_condition_df = loaddata_df.copy()\n",
    "    for condition, values in conditions.items():\n",
    "        loaddata_condition_df = loaddata_condition_df[\n",
    "            loaddata_condition_df[condition].isin(values)\n",
    "        ]\n",
    "    ## Collect corresponding single cell level features\n",
    "    sc_features = pd.DataFrame()\n",
    "    for plate in loaddata_condition_df['Metadata_Plate'].unique():\n",
    "        sc_features_parquet = SC_FEATURES_DIR / f'{plate}_sc_normalized.parquet'\n",
    "        if not sc_features_parquet.exists():\n",
    "            print(f'{sc_features_parquet} does not exist, skipping...')\n",
    "            continue \n",
    "        else:\n",
    "            sc_features = pd.concat([\n",
    "                sc_features, \n",
    "                pd.read_parquet(\n",
    "                    sc_features_parquet,\n",
    "                    columns=['Metadata_Plate', 'Metadata_Well', 'Metadata_Site', 'Metadata_Cells_Location_Center_X', 'Metadata_Cells_Location_Center_Y']\n",
    "                )\n",
    "            ])\n",
    "    ## Create patch dataset\n",
    "    pds = PatchDataset(\n",
    "        _loaddata_csv=loaddata_condition_df,\n",
    "        _sc_feature=sc_features,\n",
    "        _input_channel_keys=INPUT_CHANNEL_NAMES,\n",
    "        _target_channel_keys=TARGET_CHANNEL_NAMES,\n",
    "        _input_transform=PixelDepthTransform(src_bit_depth=16, target_bit_depth=8, _always_apply=True),\n",
    "        _target_transform=MinMaxNormalize(_normalization_factor=(2 ** 16) - 1, _always_apply=True),\n",
    "        patch_size=PATCH_SIZE,\n",
    "        verbose=False,\n",
    "        patch_generation_method=\"random_cell\",\n",
    "        n_expected_patches_per_img=50,\n",
    "        patch_generation_random_seed=42\n",
    "    )\n",
    "\n",
    "    ## Make folder for per confluence plotting\n",
    "    CONF_PLOT_DIR =  PLOT_DIR / confluence_group_name\n",
    "    CONF_PLOT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    for channel_name in TARGET_CHANNEL_NAMES:\n",
    "\n",
    "        ## Make folder for per channel plotting under confluence\n",
    "        CHANNEL_CONF_PLOT_DIR =  CONF_PLOT_DIR / channel_name\n",
    "        CHANNEL_CONF_PLOT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "        ## Cache single input/single target dataset to speed up training\n",
    "        pds.set_input_channel_keys(INPUT_CHANNEL_NAMES)\n",
    "        pds.set_target_channel_keys(channel_name)\n",
    "        cds = CachedDataset(\n",
    "            dataset=pds,\n",
    "            prefill_cache=True\n",
    "        )\n",
    "\n",
    "        print(\"Beginning FNet model training for channel: \" \\\n",
    "              f\"{channel_name} and group {confluence_group_name}\")\n",
    "        \n",
    "        ## Access relevant optuna study\n",
    "        study_path = OPTUNA_RESULTS_DIR / \\\n",
    "            f\"FNet_optimize_{channel_name}_{confluence_group_name}.joblib\"\n",
    "        if study_path.exists():\n",
    "            study = joblib.load(study_path)\n",
    "        else:\n",
    "            print(\"Optimization result not found for channel: \"\\\n",
    "                  f\"{channel_name} and group {confluence_group_name}, skipping ...\")\n",
    "        \n",
    "        ## Retrieve best parameters\n",
    "        lr = study.best_params['lr']\n",
    "        beta0 = study.best_params['beta0']\n",
    "        beta1 = study.best_params['beta1']\n",
    "        betas = (beta0, beta1)\n",
    "        patience = study.best_params['patience']\n",
    "        batch_size = study.best_params['batch_size']\n",
    "        conv_depth = study.best_params['conv_depth']\n",
    "\n",
    "        torch.manual_seed(train_seed)\n",
    "        torch.cuda.manual_seed(train_seed)\n",
    "        torch.cuda.manual_seed_all(train_seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "        model = FNet(depth=conv_depth)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "        mlflow_logger_callback = MlflowLogger(\n",
    "            name='mlflow_logger',\n",
    "            mlflow_uri=MLFLOW_DIR / 'mlruns',\n",
    "            mlflow_experiment_name=f'FNet_train_{confluence_group_name}',\n",
    "            mlflow_start_run_args={'run_name': f'FNet_train_{confluence_group_name}_{channel_name}', 'nested': True},\n",
    "            mlflow_log_params_args={\n",
    "                \"lr\": lr,\n",
    "                \"beta0\": beta0,\n",
    "                \"beta1\": beta1,\n",
    "                \"depth\": conv_depth,\n",
    "                \"patch_size\": PATCH_SIZE,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"patience\": patience,\n",
    "                \"channel_name\": channel_name,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        plot_callback = IntermediatePatchPlot(\n",
    "            name='plotter',\n",
    "            path=CHANNEL_CONF_PLOT_DIR,\n",
    "            dataset=pds, # give it the patch dataset as opposed to the cached dataset\n",
    "            plot_metrics=[SSIM(_metric_name='ssim'), PSNR(_metric_name='psnr')],\n",
    "            figsize=(20, 25),\n",
    "            every_n_epochs=5,\n",
    "            show_plot=False,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model = model,\n",
    "            optimizer = optimizer,\n",
    "            backprop_loss = torch.nn.L1Loss(), # MAE loss for backpropagation\n",
    "            dataset = pds,\n",
    "            batch_size = batch_size,\n",
    "            epochs = EPOCHS,\n",
    "            patience = patience,\n",
    "            callbacks=[mlflow_logger_callback, plot_callback],\n",
    "            metrics=metric_fns,\n",
    "            device = 'cuda',\n",
    "            early_termination_metric='L1Loss'\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        del model\n",
    "        del trainer\n",
    "\n",
    "        del mlflow_logger_callback\n",
    "        del plot_callback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speckle_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
