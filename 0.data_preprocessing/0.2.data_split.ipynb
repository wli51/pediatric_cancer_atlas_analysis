{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook generate data splits for training, heldout and evaluation dataset based on cell line, plate and seeding density\n",
    "This entire data pre-processing step (and the repo in general) will be dependent on a local pediatric_cancer_atlas_profiling repo (https://github.com/WayScience/pediatric_cancer_atlas_profiling) that is ran up to 2.feature_extraction. The config.yml file will need to be configured with the correct path to the pediatric_cancer_atlas_profiling repo for this notebook to work.\n",
    "\n",
    "This notebook relies the loaddata csv file generated from the pediatric_cancer_atlas_profiling to index images and the QC output from 0.1.filter_low_quality_sites.ipynb in this repo. It will take the loaddata csv file, remove sites marked for exclusion, and divide the loaddata csv into 3 csv files for train, heldout and evaluation dataset. \n",
    "Specifically, the U2-OS cell line on plate1 of all cell plating densities will be selected as the trianing set and one random well per seedinng density will be saved as heldout, every thing else (different cell lines across 2 plates and U2-OS on plate 2 will be used as evaluation dataset to compare differential model performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path('.').absolute().parent / \"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define paths to metadata, loaddata csvs and sc features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Access profiling repo path from config\n",
    "PROFILING_DIR = pathlib.Path(config['paths']['pediatric_cancer_atlas_profiling_path'])\n",
    "\n",
    "## Output path for the data split loaddata csvs generated in this notebook \n",
    "DATASPLIT_OUTPUT_DIR = pathlib.Path('.') / 'data_split_loaddata'\n",
    "DATASPLIT_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "## Path to platemap level metadata in pediatric_cancer_atlas_profiling repo\n",
    "# this associates Platemap-cell_line-seeding_density-well information\n",
    "platemap_csv_path = PROFILING_DIR \\\n",
    "    / \"0.download_data\" / \"metadata\" / \"platemaps\"\n",
    "assert platemap_csv_path.exists()\n",
    "\n",
    "## Path to loaddata csvs in pediatric_cancer_atlas_profiling repo\n",
    "# this associates well with image_path\n",
    "loaddata_csv_path = PROFILING_DIR \\\n",
    "    / \"1.illumination_correction\" / \"loaddata_csvs\"\n",
    "assert loaddata_csv_path.exists()\n",
    "\n",
    "## Path to QC excluded site csv file\n",
    "qc_path = pathlib.Path('.').absolute() \\\n",
    "    / \"preprocessing_output\" / \"qc_exclusion.csv\"\n",
    "assert qc_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define columns that uniquely identifies well, condition (cell_line + seeding_density) and the train U2-OS condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Whether to remove sites with low QC score\n",
    "QC = True\n",
    "\n",
    "\n",
    "## Wells are uniquely identified by the combination of these columns\n",
    "## Define columns in loaddata\n",
    "SITE_COLUMN = 'Metadata_Site'\n",
    "WELL_COLUMN = 'Metadata_Well'\n",
    "PLATE_COLUMN = 'Metadata_Plate'\n",
    "UNIQUE_IDENTIFIERS = [SITE_COLUMN, WELL_COLUMN, PLATE_COLUMN]\n",
    "\n",
    "## Condition for train and heldout data (every other condition will be left for evaluation)\n",
    "TRAIN_CONDITION_KWARGS = {\n",
    "    'cell_line': 'U2-OS',\n",
    "    'platemap_file': 'Assay_Plate1_platemap', # plate 1 only\n",
    "    'seeding_density': [1_000, 2_000, 4_000, 8_000, 12_000]\n",
    "}\n",
    "\n",
    "## Conditions are uniquely identified by the combination of keys from TRAIN_CONDITION_KWARGS\n",
    "CONDITIONS = list(TRAIN_CONDITION_KWARGS.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all barcode/platemap metadata and all loaddata csv files and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10249 sites prior to QC\n",
      "9358 sites after QC\n"
     ]
    }
   ],
   "source": [
    "## Read platemap and well cell line metadata\n",
    "barcode_df = pd.concat([pd.read_csv(f) for f in platemap_csv_path.glob('Barcode_*.csv')])\n",
    "\n",
    "## Infers from barcode_df how many plates exist, retrieve all plate metadata and merge with barcode_df\n",
    "platemap_df = pd.DataFrame()\n",
    "for platemap in barcode_df['platemap_file'].unique():\n",
    "    df = pd.read_csv(platemap_csv_path / f'{platemap}.csv')\n",
    "    df['platemap_file'] = platemap\n",
    "    platemap_df = pd.concat([platemap_df, df])    \n",
    "barcode_platemap_df = pd.merge(barcode_df, platemap_df, on='platemap_file', how='inner')\n",
    "\n",
    "## Read QC file\n",
    "remove_sites = pd.read_csv(qc_path)\n",
    "\n",
    "## Read loaddata csvs\n",
    "loaddata_df = pd.concat(\n",
    "    [pd.read_csv(f) for f in loaddata_csv_path.glob('*.csv')], \n",
    "    ignore_index=True)\n",
    "\n",
    "## Merge loaddata with barcode/platemap metadata to map condition to well\n",
    "loaddata_barcode_platemap_df = pd.merge(\n",
    "    barcode_platemap_df.rename(columns={'barcode': PLATE_COLUMN, 'well': WELL_COLUMN}),\n",
    "    loaddata_df,\n",
    "    on=[PLATE_COLUMN, WELL_COLUMN], \n",
    "    how='left')\n",
    "\n",
    "## Perform QC removal per site\n",
    "if QC:\n",
    "    print(f\"{loaddata_barcode_platemap_df.shape[0]} sites prior to QC\")\n",
    "    # Merge to correctly identify rows to be removed\n",
    "    qc_merge_df = loaddata_barcode_platemap_df.merge(\n",
    "        remove_sites, \n",
    "        on=UNIQUE_IDENTIFIERS, \n",
    "        how='left', \n",
    "        indicator=True\n",
    "        )\n",
    "\n",
    "    # Keep only rows that were NOT found in remove_sites\n",
    "    loaddata_barcode_platemap_df = qc_merge_df[qc_merge_df['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "    print(f\"{loaddata_barcode_platemap_df.shape[0]} sites after QC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537 sites for train and heldout\n",
      "8821 sites for evaluation\n"
     ]
    }
   ],
   "source": [
    "loaddata_barcode_platemap_train_df = loaddata_barcode_platemap_df.copy()\n",
    "\n",
    "## Filter load data csvs dynamically with TRAIN_CONDITION_KWARGS\n",
    "for k, v in TRAIN_CONDITION_KWARGS.items():\n",
    "    if isinstance(v, list):\n",
    "        loaddata_barcode_platemap_train_df = loaddata_barcode_platemap_train_df[loaddata_barcode_platemap_train_df[k].isin(v)]\n",
    "    else:\n",
    "        loaddata_barcode_platemap_train_df = loaddata_barcode_platemap_train_df[loaddata_barcode_platemap_train_df[k] == v]\n",
    "    if len(loaddata_barcode_platemap_train_df) == 0:\n",
    "        raise ValueError(f'No data found for {k}={v}')\n",
    "print(f\"{loaddata_barcode_platemap_train_df.shape[0]} sites for train and heldout\")\n",
    "\n",
    "## Everything else is used for eval \n",
    "loaddata_barcode_platemap_eval_df = loaddata_barcode_platemap_df.loc[\n",
    "    ~loaddata_barcode_platemap_df.index.isin(loaddata_barcode_platemap_train_df.index)\n",
    "]\n",
    "print(f\"{loaddata_barcode_platemap_eval_df.shape[0]} sites for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each unique condition combation in train/heldout split, hold out one well at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 1000} Heldout well: ['M14'] Train wells: ['M13' 'N13' 'N14']\n",
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 2000} Heldout well: ['N16'] Train wells: ['M15' 'N15' 'M16']\n",
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 4000} Heldout well: ['M17'] Train wells: ['N17' 'M18' 'N18']\n",
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 8000} Heldout well: ['M20'] Train wells: ['M19' 'N19' 'N20']\n",
      "For Condition: {'cell_line': 'U2-OS', 'platemap_file': 'Assay_Plate1_platemap', 'seeding_density': 12000} Heldout well: ['M22'] Train wells: ['M21' 'N21' 'N22']\n",
      "135 sites Heldout\n",
      "402 sites for Training\n"
     ]
    }
   ],
   "source": [
    "## Reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "## Group by seeding density and cell line (condition)\n",
    "grouped = loaddata_barcode_platemap_train_df.groupby(CONDITIONS)\n",
    "\n",
    "## Initialize lists to store holdout and train data\n",
    "heldout_list = []\n",
    "train_list = []\n",
    "\n",
    "## Iterate over each group (condition)\n",
    "for _, group in grouped:\n",
    "\n",
    "    # sample one well in each group at random\n",
    "    held_out_well = [np.random.choice(group[WELL_COLUMN].unique())]\n",
    "    train_wells = group[~group[WELL_COLUMN].isin(held_out_well)][WELL_COLUMN].unique()\n",
    "\n",
    "    # subset group into train and heldout\n",
    "    loaddata_held_out_df = group[group[WELL_COLUMN].isin(held_out_well)].copy()\n",
    "    loaddata_train_df = group[group[WELL_COLUMN].isin(train_wells)].copy()\n",
    "\n",
    "    # print which well is heldout\n",
    "    condition = group[CONDITIONS].iloc[0].to_dict()\n",
    "    print(f\"For Condition: {condition} Heldout well: {held_out_well} Train wells: {train_wells}\")\n",
    "\n",
    "    # append subset groups to lists\n",
    "    heldout_list.append(loaddata_held_out_df)\n",
    "    train_list.append(loaddata_train_df)\n",
    "\n",
    "# Concatenate the lists into final train and heldout loaddata dataframes\n",
    "loaddata_heldout_df = pd.concat(heldout_list).reset_index(drop=True)\n",
    "print(f\"{loaddata_heldout_df.shape[0]} sites Heldout\")\n",
    "loaddata_train_df = pd.concat(train_list).reset_index(drop=True)\n",
    "print(f\"{loaddata_train_df.shape[0]} sites for Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaddata_heldout_df.to_csv(DATASPLIT_OUTPUT_DIR / 'loaddata_heldout.csv')\n",
    "loaddata_train_df.to_csv(DATASPLIT_OUTPUT_DIR / 'loaddata_train.csv')\n",
    "loaddata_barcode_platemap_eval_df.to_csv(DATASPLIT_OUTPUT_DIR / 'loaddata_eval.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speckle_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
